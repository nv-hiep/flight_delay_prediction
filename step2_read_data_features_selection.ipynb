{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "step2_read_data_features_selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzsPSiRK2csBFCXUJfUP5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nv-hiep/flight_delay_prediction/blob/master/step2_read_data_features_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Boi4l8ldjJ"
      },
      "source": [
        "**Step 2: FEATURES SELECTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twluDsqOaqRW"
      },
      "source": [
        "**Open this notebook from google drive**<br>\n",
        "**Go to \"Edit\" -> \"Notebook settings\" and enable GPU.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj3eAw1OXOnB"
      },
      "source": [
        "**Connect and authorize google drive with google colab:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjjcQSpya_FR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# !ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDJptlLBvSlN"
      },
      "source": [
        "#Import Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3cWo7hhc-qO"
      },
      "source": [
        "import os\n",
        "import numpy   as np\n",
        "import pandas  as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%matplotlib inline  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgfeZe4ivlpb"
      },
      "source": [
        "# Data directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9d185J9_xQZ"
      },
      "source": [
        "data_dir    = '/content/gdrive/My Drive/data'\n",
        "%cd '/content/gdrive/My Drive/data'\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "print(current_dir)\n",
        "data_path = os.path.join(data_dir, 'flights', '')\n",
        "print(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85WkcDEhv-XY"
      },
      "source": [
        "# Read flight data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGskTmcXbLIv"
      },
      "source": [
        "df = pd.read_csv(os.path.join(data_path, 'merged_data_janfeb.csv') )\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX6d6lOTbf0Q"
      },
      "source": [
        "print(df.shape)\n",
        "print(len(df.columns))\n",
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ3tktjdp7-C"
      },
      "source": [
        "# Check null values\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3BTwigy_anm"
      },
      "source": [
        "# OK! No more null/nan values\n",
        "# Check the datatypes\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_BleyrbDQqH"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BsJo-KMYHh9"
      },
      "source": [
        "# Drop unnecessary columns "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ullwhs-xXBAe"
      },
      "source": [
        "**Drop the columns 'AIRLINE_NAME', 'YEAR', 'QUARTER' because they are not necessary. I only consider data in January and February 2017. And AIRLINE = OP_UNIQUE_CARRIER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPOb2JKW6CCR"
      },
      "source": [
        "df.drop(['AIRLINE_NAME', 'YEAR', 'QUARTER'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWgGJvB5fjaj"
      },
      "source": [
        "# Create TARGETS as classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOcMS9UNfy60"
      },
      "source": [
        "**Instead of predicting the delay time in minutes, I'll predict in delay intervals:**\n",
        "\n",
        "**0: ARR_DELAY <= 0 for no delay**\n",
        "\n",
        "**1: 0 < ARR_DELAY <= 30 (minutes)**\n",
        "\n",
        "**2: 30 < ARR_DELAY <= 60 (minutes)**\n",
        "\n",
        "**3: 60 < ARR_DELAY <= 120 (minutes)**\n",
        "\n",
        "**43: 120 < ARR_DELAY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RuCfjs3khq7"
      },
      "source": [
        "# Create delayed labels, if the flights are delayed more than 30 minutes (delay_thresh)\n",
        "df['DELAYED'] = df['ARR_DELAY'].apply(lambda x: 0 if x <= 0 else 1 if (x > 0 and x <= 30) else 2 if (x > 30 and x <= 60) else 3 if (x > 60 and x <= 120) else 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn3_owJ9Vi7G"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4DLpYqMlJXl"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4I_0q-lk5YV"
      },
      "source": [
        "# Convert time of Departure and arrival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uat0WekJwHjo"
      },
      "source": [
        "def convert_time(x):\n",
        "  x = x/100.\n",
        "  return round( int(x) + (x - int(x))*100/60, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRrLbpMUDbH0"
      },
      "source": [
        "# The actual departure time\n",
        "df['CRS_DEP_TIME'] = df['CRS_DEP_TIME'].apply(convert_time)\n",
        "df['CRS_DEP_TIME'] = df['CRS_DEP_TIME'].apply(lambda x:0 if x==24 else x)\n",
        "\n",
        "df['DEP_TIME'] = df['DEP_TIME'].apply(convert_time)\n",
        "df['DEP_TIME'] = df['DEP_TIME'].apply(lambda x:0 if x==24 else x)\n",
        "\n",
        "# The actual arrival time\n",
        "df['CRS_ARR_TIME'] = df['CRS_ARR_TIME'].apply(convert_time)\n",
        "df['CRS_ARR_TIME'] = df['CRS_ARR_TIME'].apply(lambda x:0 if x==24 else x)\n",
        "\n",
        "df['ARR_TIME'] = df['ARR_TIME'].apply(convert_time)\n",
        "df['ARR_TIME'] = df['ARR_TIME'].apply(lambda x:0 if x==24 else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKpgOL-YYV-V"
      },
      "source": [
        "# Label - Encoding the categorical feautures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I57AewJYfQY"
      },
      "source": [
        "Here I should use One-hot-coding method, however this method will produce many more columns, so it would take a long time to train.\n",
        "\n",
        "I will use the Label-encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txdGc5ko28sF"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "lb_make = LabelEncoder()\n",
        "df['ORIGIN'] = lb_make.fit_transform(df['ORIGIN'])\n",
        "df['DEST'] = lb_make.transform(df['DEST'])\n",
        "\n",
        "df['OP_UNIQUE_CARRIER'] = lb_make.fit_transform(df['OP_UNIQUE_CARRIER'])\n",
        "df['TAIL_NUM'] = lb_make.fit_transform(df['TAIL_NUM'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VzCXskahXCV"
      },
      "source": [
        "# dummy_fields = ['DAY_OF_WEEK', 'OP_UNIQUE_CARRIER', 'TAIL_NUM', 'ORIGIN', 'DEST']\n",
        "# for x in dummy_fields:\n",
        "#   dummy = pd.get_dummies(df[x], drop_first=False, prefix=x)\n",
        "#   df = pd.concat( [df, dummy], axis=1)\n",
        "# df_sub.drop(dummy_fields, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcZvuJ9SfUiV"
      },
      "source": [
        "# Remove Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86EpaVPfUiV"
      },
      "source": [
        "# Missing values (in percent)\n",
        "df_missing = (df.isnull().sum() / len(df)).sort_values(ascending = False)\n",
        "df_missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zliXI1PufUiV"
      },
      "source": [
        "# Identify missing values above threshold\n",
        "df_missing = df_missing.index[df_missing > 0.75]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0GE5V-bfUiV"
      },
      "source": [
        "print('There are %d columns with more than 75%% missing values' % len(df_missing))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANLfJYJ0fUiW"
      },
      "source": [
        "# Let's drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes.\n",
        "df.drop(df_missing, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSDqLoVRZQmb"
      },
      "source": [
        "# Drop Correlated Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBnzZvzCz2K0"
      },
      "source": [
        "Collinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set. Clearly, these are three things we want to increase, so removing collinear variables is a useful step. We will establish an admittedly arbitrary threshold for removing collinear variables, and then remove one out of any pair of variables that is above that threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag51-gimzlgE"
      },
      "source": [
        "# Threshold for removing correlated variables\n",
        "threshold = 0.9\n",
        "\n",
        "# Correlation matrix with absolute values\n",
        "corr_matrix = df.corr().abs()\n",
        "corr_matrix.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNTF6NxHzWDJ"
      },
      "source": [
        "# Upper triangle of correlations\n",
        "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Lower triangle of correlations\n",
        "lower_matrx = corr_matrix.where(np.tril(np.ones(corr_matrix.shape), k=-1).astype(np.bool))\n",
        "lower_matrx.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJOygMd5xIuU"
      },
      "source": [
        "plt.figure(figsize=(14,10))\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "plt.title('Pearson Correlation of Features', y=1.05, size=20, color='R')\n",
        "\n",
        "sns.heatmap(lower_matrx, linewidths=0.1, vmax=1.0,\n",
        "            square=True, cmap=plt.cm.RdBu_r, linecolor='white', annot=True);\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z2Slsqx8xNQ"
      },
      "source": [
        "# Select columns with correlations above threshold\n",
        "features_to_drop = [column for column in lower_matrx.columns if any(lower_matrx[column] > threshold)]\n",
        "\n",
        "print('There are %d columns to remove.' % (len(features_to_drop)))\n",
        "features_to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rShiUOo0GYTF"
      },
      "source": [
        "df.drop(features_to_drop, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTTrsL6GuFp"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QdF-YvhLes0"
      },
      "source": [
        "# Feature Selection using Feature Importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNp_9WwDL2oX"
      },
      "source": [
        "Perform a feature removal by first removing all zero importance features from the model. If this leaves too many features, then we can consider removing the features with the lowest importance. We will use a Gradient Boosted Model from the LightGBM library to assess feature importances. If you're used to the Scikit-Learn library, the LightGBM library has an API that makes deploying the model very similar to using a Scikit-Learn model.\n",
        "\n",
        "Since the LightGBM model does not need missing values to be imputed, we can directly fit on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbW5L44aieYu"
      },
      "source": [
        "**NOTE:**\n",
        "\n",
        "**To save time, I will use ONLY 20,000 samples of January 2017 for the feature selection and 5000 samples of January 2017 the training in the next step.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kok-x1RL1TJ"
      },
      "source": [
        "# Modeling \n",
        "import lightgbm as lgb\n",
        "\n",
        "df = df[ df.MONTH == 1 ]\n",
        "\n",
        "y_train = df['DELAYED']\n",
        "df.drop( ['DELAYED', 'ARR_DELAY', 'MONTH'], axis=1, inplace=True )\n",
        "\n",
        "X_train = df.copy()\n",
        "del df\n",
        "\n",
        "X_train = X_train[:20_000]\n",
        "y_train = y_train[:20_000]\n",
        "\n",
        "# Initialize an empty array to hold feature importances\n",
        "feature_importances = np.zeros(X_train.shape[1])\n",
        "\n",
        "# Create the model with several hyperparameters\n",
        "model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n",
        "\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZWwiNHeQcxi"
      },
      "source": [
        "# Import\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics         import roc_auc_score\n",
        "\n",
        "# Fit the model twice to avoid overfitting\n",
        "nruns = 2\n",
        "for i in range(nruns):\n",
        "    \n",
        "    # Split into training and validation set\n",
        "    train_features, valid_features, train_y, valid_y = train_test_split(X_train, y_train, test_size = 0.25, random_state = i)\n",
        "    \n",
        "    # Train using early stopping\n",
        "    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
        "              eval_metric = 'logloss', verbose = 200)\n",
        "    \n",
        "    # Record the feature importances\n",
        "    feature_importances += model.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es77Xhd8S5Z5"
      },
      "source": [
        "# Make sure to average feature importances! \n",
        "feature_importances = feature_importances / nruns\n",
        "feature_importances = pd.DataFrame({'feature': list(X_train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
        "\n",
        "feature_importances.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP8xWpSyWGhF"
      },
      "source": [
        "# Find the features with zero importance\n",
        "zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
        "\n",
        "print('There are %d features with 0.0 importance' % len(zero_features))\n",
        "print(zero_features)\n",
        "\n",
        "feature_importances.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAoxxSbfWxdL"
      },
      "source": [
        "def plot_feature_importances(df, threshold = 0.9):\n",
        "    \"\"\"\n",
        "    Plot 15 most important features and the cumulative importance of features.\n",
        "    Prints the number of features needed to reach threshold cumulative importance.\n",
        "    \n",
        "    Parameters\n",
        "    --------\n",
        "    df : dataframe\n",
        "        Dataframe of feature importances. Columns must be feature and importance\n",
        "    threshold : float, default = 0.9\n",
        "        Threshold for prining information about cumulative importances\n",
        "        \n",
        "    Return\n",
        "    --------\n",
        "    df : dataframe\n",
        "        Dataframe ordered by feature importances with a normalized column (sums to 1)\n",
        "        and a cumulative importance column\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    plt.rcParams['font.size'] = 18\n",
        "    \n",
        "    # Sort features according to importance\n",
        "    df = df.sort_values('importance', ascending = False).reset_index()\n",
        "    \n",
        "    # Normalize the feature importances to add up to one\n",
        "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
        "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
        "\n",
        "    # Make a horizontal bar chart of feature importances\n",
        "    plt.figure(figsize = (10, 6))\n",
        "    ax = plt.subplot()\n",
        "    \n",
        "    # Need to reverse the index to plot most important on top\n",
        "    ax.barh(list(reversed(list(df.index[:15]))), \n",
        "            df['importance_normalized'].head(15), \n",
        "            align = 'center', edgecolor = 'k')\n",
        "    \n",
        "    # Set the yticks and labels\n",
        "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
        "    ax.set_yticklabels(df['feature'].head(15))\n",
        "    \n",
        "    # Plot labeling\n",
        "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
        "    plt.show()\n",
        "    \n",
        "    # Cumulative importance plot\n",
        "    plt.figure(figsize = (8, 6))\n",
        "    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n",
        "    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n",
        "    plt.title('Cumulative Feature Importance');\n",
        "    plt.show();\n",
        "    \n",
        "    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
        "    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYMQpuphW0--"
      },
      "source": [
        "# Normalized feature importance\n",
        "norm_feature_importances = plot_feature_importances(feature_importances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvHpJ3OwXVlQ"
      },
      "source": [
        "X_train.drop( zero_features, axis=1, inplace=True )\n",
        "print('Training shape: ', X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUXKOaPIYYHz"
      },
      "source": [
        "At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpOaJ0zSYif6"
      },
      "source": [
        "def identify_zero_importance_features(X, y, iterations = 2):\n",
        "    \"\"\"\n",
        "    Identify zero importance features in a training dataset based on the \n",
        "    feature importances from a gradient boosting model. \n",
        "    \n",
        "    Parameters\n",
        "    --------\n",
        "    X : dataframe\n",
        "        Training features\n",
        "        \n",
        "    y : np.array\n",
        "        Labels for training data\n",
        "        \n",
        "    iterations : integer, default = 2\n",
        "        Number of cross validation splits to use for determining feature importances\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize an empty array to hold feature importances\n",
        "    feature_importances = np.zeros(X.shape[1])\n",
        "\n",
        "    # Create the model with several hyperparameters\n",
        "    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n",
        "    \n",
        "    # Fit the model multiple times to avoid overfitting\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # Split into training and validation set\n",
        "        train_features, valid_features, train_y, valid_y = train_test_split(X, y, test_size = 0.25, random_state = i)\n",
        "\n",
        "        # Train using early stopping\n",
        "        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
        "                  eval_metric = 'logloss', verbose = 200)\n",
        "\n",
        "        # Record the feature importances\n",
        "        feature_importances += model.feature_importances_ / iterations\n",
        "    \n",
        "    feature_importances = pd.DataFrame({'feature': list(X.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
        "    \n",
        "    # Find the features with zero importance\n",
        "    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
        "    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n",
        "    \n",
        "    return zero_features, feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhyuWfJYssX"
      },
      "source": [
        "second_round_zero_features, feature_importances = identify_zero_importance_features(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJaQoMidZS5y"
      },
      "source": [
        "norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHEQcR36bhit"
      },
      "source": [
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjzgJeUucjoI"
      },
      "source": [
        "norm_feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdNaxFZObyxH"
      },
      "source": [
        "# Threshold for cumulative importance\n",
        "threshold = 0.95\n",
        "\n",
        "\n",
        "# Extract the features to keep\n",
        "features_to_keep = list(norm_feature_importances[norm_feature_importances['cumulative_importance'] < threshold]['feature'])\n",
        "\n",
        "print('Number of feautures of keep: ', len(features_to_keep))\n",
        "print('Features to keep: ', features_to_keep)\n",
        "\n",
        "# Create new datasets with smaller features\n",
        "X_keep = X_train[features_to_keep]\n",
        "X_keep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_FXhJDPcTWf"
      },
      "source": [
        "X_keep = X_keep.assign(DELAYED = y_train)\n",
        "X_keep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVieS5MHkOHW"
      },
      "source": [
        "X_keep.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5CVTraCl_N_"
      },
      "source": [
        "# Re-arrange\n",
        "X_keep[ ['DAY_OF_MONTH', 'OP_CARRIER_FL_NUM', 'TAIL_NUM', 'ORIGIN', 'DEST',\n",
        "       'DEP_TIME', 'ARR_TIME', 'CRS_ARR_TIME', 'DISTANCE', 'TAXI_OUT',\n",
        "       'TAXI_IN', 'CARRIER_DELAY', 'NAS_DELAY', 'LATE_AIRCRAFT_DELAY', 'DELAYED']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2XD6F3hpYKe"
      },
      "source": [
        "# Save the cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwV4gZcJpR2b"
      },
      "source": [
        "X_keep.to_csv( os.path.join(data_path, 'cleaned_data_jan_20klines.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}