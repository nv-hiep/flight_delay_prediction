{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "step4_conclusion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTTBGJ9Iunyu2L4FQ3KN2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nv-hiep/flight_delay_prediction/blob/master/step4_conclusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akoOOZFvp2Y_"
      },
      "source": [
        "**Step 4: Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJk2gHkosPAv"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlVh9Fxap9m8"
      },
      "source": [
        "I have used a dataset of airline information to predict how late flights will be. A flight only counts as late if it is more than 30 minutes late.\n",
        "\n",
        "Instead of predicting the delay time in minutes, I'll predict in delay intervals:\n",
        "\n",
        "0: ARR_DELAY <= 0 for no delay\n",
        "\n",
        "1: 0 < ARR_DELAY <= 30 (minutes)\n",
        "\n",
        "2: 30 < ARR_DELAY <= 60 (minutes)\n",
        "\n",
        "3: 60 < ARR_DELAY <= 120 (minutes)\n",
        "\n",
        "4: 120 < ARR_DELAY\n",
        "\n",
        "To do this I apply different models (Naive Bayes, Logistic Regression, Decision Tree, Random Forest, Gradient Boosting and SVM) for a multiclass classification.\n",
        "\n",
        "I used the data for January and February 2017 for the analysis, but only 1000 samples for training the models.\n",
        "\n",
        "For each model,I performed the following procedure:\n",
        "\n",
        "1. Read the data.\n",
        "\n",
        "2. Split the data into training and test sets.\n",
        "\n",
        "3. Run the model on the training and test sets, record the accuracy score.\n",
        "\n",
        "4. Run the model on the training and test sets, using SelectKBest for Feature Selection, record the accuracy score.\n",
        "\n",
        "5. Run the model on the training and test sets, using PCA for Feature Selection, record the accuracy score.\n",
        "\n",
        "6. Run the model on the training and test sets, using RFE for Feature Selection, record the accuracy score.\n",
        "\n",
        "7. Compare the accuracy scores and choose the best method for Feature Selection.\n",
        "\n",
        "8. Get best parameters of the model using GridSearchCV, record the accuracy score.\n",
        "\n",
        "9. Get best parameters of the model using Hyperopt, record the accuracy score.\n",
        "\n",
        "10. Compare the accuracy scores to choose Hyperopt or GridSearch for obtaining the best parameters.\n",
        "\n",
        "11. Perform a 10-fold cross-validation with the best parameters, record the accuracy score which will be used to compare the performances of different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsAwgPohsSjo"
      },
      "source": [
        "# Comparison of the model performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZeCP8uvr3hS"
      },
      "source": [
        "The 10-fold cross validation procedure is used to evaluate each algorithm, importantly configured with the same random seed to ensure that the same splits to the training data are performed and that each algorithms is evaluated in precisely the same way.\n",
        "\n",
        "Running the above prcedures provides a list of each algorithm, the mean accuracy and the standard deviation accuracy. As shown below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-ZyCUybsmr3"
      },
      "source": [
        "1. **Naive Bayesi Classifier**: \n",
        "*   Results from cross-validation: [0.61 0.59 0.6  0.62 0.58 0.59 0.61 0.63 0.57 0.6 ]\n",
        "*   Accuracy: 0.6000 (+/- 0.0173)\n",
        "\n",
        "\n",
        "2. **Logistic Regression Classifier**: \n",
        "*   Results from cross-validation: [0.79 0.84 0.82 0.76 0.78 0.77 0.83 0.78 0.77 0.81]\n",
        "*   Accuracy: 0.7950 (+/- 0.0266)\n",
        "\n",
        "\n",
        "3.   **Decision Tree Classifier**\n",
        "*   Results from cross-validation [0.78 0.81 0.73 0.77 0.7  0.78 0.86 0.72 0.8  0.78]\n",
        "*   Accuracy: 0.7730 (+/- 0.0445)\n",
        "\n",
        "\n",
        "4.   **Random Forest Classifier**:\n",
        "*   Results from cross-validation [0.78 0.85 0.75 0.77 0.76 0.82 0.84 0.82 0.79 0.82]\n",
        "*   Accuracy: 0.8000 (+/- 0.0329)\n",
        "\n",
        "\n",
        "5.   **Gradient Boosting Classifier**:\n",
        "*   Results from cross-validation: [0.82 0.89 0.83 0.83 0.81 0.8  0.84 0.85 0.82 0.83]\n",
        "*   Accuracy: 0.8320 (+/- 0.0236)\n",
        "\n",
        "6.   **SVM**:\n",
        "*   Results from cross-validation: [0.82 0.79 0.71 0.83 0.77 0.75 0.81 0.78 0.76 0.8 ]\n",
        "*   Accuracy: 0.7820 (+/- 0.0343)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cTcP8MIvNnV"
      },
      "source": [
        "A box plot showing the spread of the accuracy scores across each cross validation fold for each algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1xQLHRBvMyZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results = [\n",
        "           [0.61, 0.59, 0.6, 0.62, 0.58, 0.59, 0.61, 0.63, 0.57, 0.6 ],\n",
        "           [0.79, 0.84, 0.82, 0.76, 0.78, 0.77, 0.83, 0.78, 0.77, 0.81],\n",
        "           [0.78, 0.81, 0.73, 0.77, 0.7, 0.78, 0.86, 0.72, 0.8, 0.78],\n",
        "           [0.78, 0.85, 0.75, 0.77, 0.76, 0.82, 0.84, 0.82, 0.79, 0.82],\n",
        "           [0.82, 0.89, 0.83, 0.83, 0.81, 0.8, 0.84, 0.85, 0.82, 0.83],\n",
        "           [0.82, 0.79, 0.71, 0.83, 0.77, 0.75, 0.81, 0.78, 0.76, 0.8] ]\n",
        "\n",
        "columns=['Naive Bayes', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'SVM']\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure( figsize=(10,6) )\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(columns)\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_ylim(0.5,1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuT0G5nvU5N"
      },
      "source": [
        "- These results suggest that both **Gradient Boosting** and **Random Forest classifiers** are worthy of further study on this problem.\n",
        "\n",
        "- The score from the **Logistic Regression Classifier** is comparable with that of **Random forest Classifier**. And the score of **Decision Tree** is slightly lower, but comparable with that of **SVM**.\n",
        "\n",
        "- The accuracy of the **Naive Bayes classifier** is the worst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak0RrZ5hEcOb"
      },
      "source": [
        "- Random Forest and Gradient Boosting are members of the **ensemble method**. They both are built from sets of decision trees. While random forest builds each tree independently, gradient boosting builds one tree at a time. While random forests combine results at the end of the process (by averaging or \"majority votes\"), gradient boosting combines results along the way.\n",
        "\n",
        "- If carefully tuning parameters, gradient boosting can result in better performance than random forests in classification. However, gradient boosting may not be a good choice if a lot of noise presents in the data, as it can result in overfitting. They also tend to be harder to tune than random forests.\n",
        "\n",
        "- Random forests perform well for multi-class object detection, which tends to have a lot of statistical noise. Gradient Boosting performs well for unbalanced data (unbalanced data refers to classification problems where we have unequal instances for different classes).\n",
        "\n",
        "- Virtually any 'ensemble method' will do better than a simple Naive Bayes in classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtcErEP6OXCE"
      },
      "source": [
        "- Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically good for two-class ones. For multiclass problem you may need to reduce it into multiple binary classification problems.\n",
        "\n",
        "- Random Forest works well with a mixture of numerical and categorical features. When features are on the various scales, it is also fine. Roughly speaking, with Random Forest you can use data as they are\n",
        "\n",
        "- SVM maximizes the \"margin\" and thus relies on the concept of \"distance\" between different points. It is up to you to decide if \"distance\" is meaningful. As a consequence, one-hot encoding for categorical features is a must-do. Further, min-max or other scaling is highly recommended at preprocessing step. **In this analysis, I should have done the one-hot encoding, however it takes long time to execute. I used Label-Encoding for the categorical features. So, one should one-hot encode the categorical features before training the models**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvu12MiXPz5m"
      },
      "source": [
        "- In Logistic Regression, features should be scaled and normalized unlike in Random Forest, they are unaffected. **Here, while performing the LR, I didn't scale and normalize the features. This should have affected the accuracy of the LR**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfow8Qu-RRjY"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BKDi-K_REEn"
      },
      "source": [
        "- I'd select Gradient Boosting and Random Forest classifiers for further consideration in this problem, since they obtained the best accuracy scores (80\\%) when predicting the arrival delay.\n",
        "\n",
        "- For other algorithms, it'd be better to scale and normalize data first since some algorithms are sensitive to it.\n",
        "\n",
        "- Since the results also depend on the random state, we should use different values and keep the average accuracy for each machine learning algorithm.\n",
        "\n",
        "- Use one-hot encoding for the categorical features.\n",
        "\n",
        "- For complex or small datasets, if we have the resources, repeated k-fold cross validation is preferred.\n",
        "\n",
        "- On very large datasets, a train-test split may be sufficient. I should use larger dataset (by including more data of years/months)."
      ]
    }
  ]
}